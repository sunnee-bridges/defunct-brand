[build]
  command = "npm run build"
  publish = "dist"

[build.environment]
  NODE_VERSION = "20"

# ---------- Serverless functions ----------
[functions]
  node_bundler = "esbuild"
  directory = "netlify/functions"

# ---------- Redirects ----------

# API routes
[[redirects]]
  from = "/api/brands"
  to = "/.netlify/functions/brands-get"
  status = 200
  force = true

[[redirects]]
  from = "/api/hello"
  to = "/.netlify/functions/hello"
  status = 200
  force = true

# Canonical apex (www -> apex)
[[redirects]]
  from = "https://www.vanishedbrands.com/*"
  to   = "https://vanishedbrands.com/:splat"
  status = 301
  force = true

# --- Brand-specific status handling ---

# 410 for intentionally removed legacy slugs (*.public)
[[redirects]]
  from = "/brand/*.public"
  to = "/410.html"
  status = 410
  force = true

[[redirects]]
  from = "/brand/*.public/"
  to = "/410.html"
  status = 410
  force = true

# Trailing slash consistency for brand & category
# (runs after the specific 410s above)
[[redirects]]
  from = "/brand/:slug"
  to = "/brand/:slug/"
  status = 301

[[redirects]]
  from = "/category/:slug"
  to = "/category/:slug/"
  status = 301

# Unknown brand paths â†’ hard 404 page
# (put this AFTER the 410 and 301 rules above)
[[redirects]]
  from = "/brand/*"
  to = "/404.html"
  status = 404

# Download route proxy -> function
[[redirects]]
  from = "/download/:token"
  to = "/.netlify/functions/download-link/:token"
  status = 200
  force = true

# Global catch-all to real 404 page
# (keep this LAST)
[[redirects]]
  from = "/*"
  to = "/404.html"
  status = 404

# ---------- Headers ----------

# Serve HTML as UTF-8
[[headers]]
  for = "/*.html"
  [headers.values]
    Content-Type = "text/html; charset=utf-8"
    X-Content-Type-Options = "nosniff"
    X-Frame-Options = "DENY"
    Referrer-Policy = "no-referrer-when-downgrade"

# Explicit robots for error documents
[[headers]]
  for = "/404.html"
  [headers.values]
    X-Robots-Tag = "noindex, follow"

[[headers]]
  for = "/410.html"
  [headers.values]
    X-Robots-Tag = "noindex, follow"

# JSON as UTF-8
[[headers]]
  for = "/**/*.json"
  [headers.values]
    Content-Type = "application/json; charset=utf-8"
    X-Content-Type-Options = "nosniff"

# Cache hashed static assets (Astro outputs under /_astro)
[[headers]]
  for = "/_astro/*"
  [headers.values]
    Cache-Control = "public, max-age=31536000, immutable"

# (Optional) also cache any legacy /assets/* if present
[[headers]]
  for = "/assets/*"
  [headers.values]
    Cache-Control = "public, max-age=31536000, immutable"

# Public sample CSV: serve correctly & allow moderate caching
[[headers]]
  for = "/data/brands-sample.csv"
  [headers.values]
    Content-Type = "text/csv; charset=utf-8"
    Cache-Control = "public, max-age=604800" # 7 days

# Keep utility/buy pages out of search
[[headers]]
  for = "/buy/"
  [headers.values]
    X-Robots-Tag = "noindex, nofollow"

# Keep download endpoints out of search & caches
[[headers]]
  for = "/download/*"
  [headers.values]
    X-Robots-Tag = "noindex, nofollow"
    Cache-Control = "no-store"

# Sitemaps: serve as XML and let caches keep for ~1h
[[headers]]
  for = "/sitemap*.xml"
  [headers.values]
    Content-Type = "application/xml; charset=utf-8"
    Cache-Control = "public, max-age=3600"

# ---------- Deploy Preview SEO guard ----------
[context.deploy-preview]
  command = "npm run build"

[[context.deploy-preview.headers]]
  for = "/*"
  [context.deploy-preview.headers.values]
    X-Robots-Tag = "noindex, nofollow"
